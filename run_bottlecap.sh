torchrun --standalone --nproc_per_node=1 train_bottlecap.py \
  --input_bin "data/fineweb10B/fineweb_train_*.bin" \
  --input_val_bin "data/fineweb10B/fineweb_val_*.bin" \
  --output_dir pylog124M_bottlecap \
  --model d12 \
  --batch_size 32 \
  --grad_accumulation_steps 32 \
  --sequence_length 256 \
  --curriculum_seq_lens 128,256,1024 \
  --curriculum_iters 0,400,1000 \
  --stochastic_depth_prob 0.03 \
  --stochastic_depth_anneal 800 \
  --adaptive_softmax \
  --asoft_cutoffs "2000,10000" \
  --asoft_div_value 4.0 \
  --val_loss_every 128 \
  --val_batch_size 16 \
  --val_sequence_length 1024 \
  --num_iterations 4000 \
  --weight_decay 0.05 \
  --learning_rate 0.0026 \
  --warmup_iters 128 \
  --warmdown_iters 2048 \
  --target_val_loss 3.3821 \
  --log_wandb
